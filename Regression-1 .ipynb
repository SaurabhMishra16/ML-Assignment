{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7684b1d6-33aa-40a7-81ad-8e3e7cc152eb",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977bca7a-9ee2-40d0-a2d8-77298f550181",
   "metadata": {},
   "source": [
    "Simple Linear Regression vs. Multiple Linear Regression:\n",
    "\n",
    "Simple Linear Regression:\n",
    "1. Definition:\n",
    "   Simple linear regression is a statistical method used to model the relationship between a single independent variable (predictor) and a dependent variable (response). It aims to find a linear equation that best describes how changes in the independent variable affect changes in the dependent variable.\n",
    "\n",
    "2. Equation:\n",
    "   The equation for simple linear regression is typically represented as:\n",
    "   Y = b0 + b1*X + ε\n",
    "   Where:\n",
    "   - Y is the dependent variable.\n",
    "   - X is the independent variable.\n",
    "   - b0 is the y-intercept (constant).\n",
    "   - b1 is the slope coefficient.\n",
    "   - ε represents the error term.\n",
    "\n",
    "3. Example:\n",
    "   Let's say you want to predict a person's weight (Y) based on their height (X). You collect data on the heights and weights of several individuals and use simple linear regression to find the equation that best fits the relationship between height and weight.\n",
    "\n",
    "Multiple Linear Regression:\n",
    "1. Definition:\n",
    "   Multiple linear regression is an extension of simple linear regression, used when there are two or more independent variables that may collectively influence a dependent variable. It models the relationship between the dependent variable and multiple predictors by fitting a linear equation.\n",
    "\n",
    "2. Equation:\n",
    "   The equation for multiple linear regression can be expressed as:\n",
    "   Y = b0 + b1*X1 + b2*X2 + ... + bn*Xn + ε\n",
    "   Where:\n",
    "   - Y is the dependent variable.\n",
    "   - X1, X2, ..., Xn are the independent variables (predictors).\n",
    "   - b0 is the y-intercept (constant).\n",
    "   - b1, b2, ..., bn are the coefficients for the independent variables.\n",
    "   - ε represents the error term.\n",
    "\n",
    "3. Example:\n",
    "   Suppose you want to predict a house's selling price (Y) based on various factors such as square footage (X1), number of bedrooms (X2), and distance to the nearest school (X3). In this case, you would use multiple linear regression to build a model that takes into account all these predictor variables to estimate the house's price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d29bd-e572-41f8-a8ad-2fb147258f91",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e20d10-f633-4058-8218-90672aec3305",
   "metadata": {},
   "source": [
    "Linear regression is a powerful statistical technique, but it comes with certain assumptions that need to hold for the results to be valid and reliable. These assumptions are:\n",
    "\n",
    "1. **Linearity**: The relationship between the independent variables (predictors) and the dependent variable (response) should be linear. This means that changes in the predictors should lead to proportional changes in the response variable.\n",
    "\n",
    "2. **Independence of Errors**: The errors or residuals (the differences between the observed values and the predicted values) should be independent of each other. In other words, the error for one data point should not be influenced by the error of another data point.\n",
    "\n",
    "3. **Homoscedasticity**: Homoscedasticity refers to the constant variance of the errors across all levels of the predictors. It means that the spread of residuals should be roughly the same throughout the range of predictor values. If there's a funnel-like pattern in a plot of residuals against predicted values, it may indicate heteroscedasticity, which violates this assumption.\n",
    "\n",
    "4. **Normality of Errors**: The errors should be normally distributed. This assumption implies that the residuals should follow a normal distribution with a mean of zero.\n",
    "\n",
    "5. **No or Little Multicollinearity**: In multiple linear regression, the independent variables should not be highly correlated with each other. High multicollinearity can make it difficult to discern the individual effects of predictors on the dependent variable.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can use various diagnostic tools and techniques:\n",
    "\n",
    "1. **Residual Plots**: Create residual plots, such as a scatterplot of residuals against predicted values. These plots can help you visually assess linearity, independence of errors, and homoscedasticity.\n",
    "\n",
    "2. **Normality Tests**: Perform statistical tests for normality on the residuals, such as the Shapiro-Wilk test or visual inspection through histograms and Q-Q plots.\n",
    "\n",
    "3. **Durbin-Watson Statistic**: This statistic can help you assess the independence of errors. A value close to 2 suggests no autocorrelation, while values significantly greater or less than 2 indicate positive or negative autocorrelation, respectively.\n",
    "\n",
    "4. **VIF (Variance Inflation Factor)**: Calculate the VIF for each predictor to check for multicollinearity. High VIF values (typically above 5 or 10) indicate a problem with multicollinearity.\n",
    "\n",
    "5. **Cook's Distance**: This metric helps identify influential data points that may disproportionately affect the regression results. Large Cook's distances suggest potential outliers.\n",
    "\n",
    "6. **Heteroscedasticity Tests**: Use formal tests like the Breusch-Pagan test or the White test to detect heteroscedasticity.\n",
    "\n",
    "7. **Transformations**: If assumptions are violated, you may need to apply data transformations (e.g., logarithmic or Box-Cox transformations) to make the data meet the assumptions better.\n",
    "\n",
    "8. **Outlier Detection**: Identify and handle outliers in your dataset, as they can significantly impact the assumptions and results of linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da073c-5f20-47cc-8183-8ba322f4c64a",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bef032-b1eb-4b96-97d8-823ecb125597",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept are essential components that help us understand and interpret the relationship between the independent variable(s) and the dependent variable. Here's how to interpret them:\n",
    "\n",
    "1. **Slope (Coefficient of the Independent Variable):** The slope represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X), holding all other variables constant. In other words, it quantifies the rate of change in Y as X changes.\n",
    "\n",
    "   - If the slope is positive (greater than 0), it indicates that there is a positive linear relationship between the independent and dependent variables. As X increases, Y also increases.\n",
    "   - If the slope is negative (less than 0), it indicates a negative linear relationship. As X increases, Y decreases.\n",
    "   - If the slope is zero, it suggests no linear relationship between X and Y.\n",
    "\n",
    "2. **Intercept:** The intercept (often denoted as \"b\" or \"intercept constant\") represents the predicted value of the dependent variable (Y) when all independent variables (X) are set to zero. It is the point where the regression line intersects the Y-axis. \n",
    "\n",
    "Now, let's illustrate this with a real-world scenario:\n",
    "\n",
    "**Scenario:** Salary Prediction based on Years of Experience\n",
    "\n",
    "Suppose you are an HR manager, and you want to predict an employee's salary based on their years of experience. You collect data on several employees and perform a linear regression analysis. Your regression equation is:\n",
    "\n",
    "Salary = Intercept + (Slope * Years of Experience)\n",
    "\n",
    "1. **Interpretation of Intercept:** \n",
    "   - Intercept represents the predicted salary when an employee has zero years of experience. In this context, it might not make much sense since no one has zero years of experience when it comes to employment. However, it is essential for the mathematical model. If the intercept is $40,000, it suggests that a hypothetical employee with zero years of experience is predicted to have a starting salary of $40,000.\n",
    "\n",
    "2. **Interpretation of Slope:** \n",
    "   - Let's say the slope of your regression line is $5,000. This means that for each additional year of experience, an employee's salary is expected to increase by $5,000, assuming all other factors (such as education, industry, location, etc.) remain constant.\n",
    "\n",
    "So, if you have an employee with 3 years of experience, you can predict their salary using the equation:\n",
    "\n",
    "Salary = Intercept + (Slope * Years of Experience)\n",
    "Salary = $40,000 + ($5,000 * 3) = $55,000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f1544-5589-46b0-b273-e66ef6edda2c",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37c810-d948-4822-b1f4-ab0cff495b0f",
   "metadata": {},
   "source": [
    "Gradient descent is a fundamental optimization algorithm used in machine learning and other optimization problems. It is especially prevalent in training machine learning models, particularly those based on neural networks. The concept of gradient descent revolves around the idea of finding the minimum (or maximum) of a function by iteratively adjusting its parameters.\n",
    "\n",
    "Here's a step-by-step explanation of gradient descent and its role in machine learning:\n",
    "\n",
    "1. **Objective Function:** In machine learning, you often have an objective function that you want to minimize or maximize. This function is typically a mathematical expression that quantifies the error or cost of your model's predictions compared to the actual data. The goal is to find the set of model parameters (weights and biases) that minimize this objective function.\n",
    "\n",
    "2. **Gradient:** The gradient of a function is a vector that points in the direction of the steepest increase in the function's value. In other words, it tells you how the function value changes as you make small adjustments to its parameters. To minimize the function, you want to move in the opposite direction of the gradient.\n",
    "\n",
    "3. **Gradient Descent Algorithm:**\n",
    "   - **Initialization:** Start with an initial guess for the model's parameters. These could be random values or some predefined values.\n",
    "   - **Compute Gradient:** Calculate the gradient of the objective function with respect to the model parameters at the current parameter values.\n",
    "   - **Update Parameters:** Adjust the model parameters in the opposite direction of the gradient to minimize the objective function. This is done by subtracting a fraction of the gradient from the current parameter values. The fraction is called the learning rate and determines the step size.\n",
    "   - **Repeat:** Continue the process iteratively, recalculating the gradient and updating the parameters until a stopping criterion is met. Common stopping criteria include a fixed number of iterations or convergence when the change in parameters becomes very small.\n",
    "\n",
    "4. **Learning Rate:** The learning rate is a hyperparameter that controls the step size in each iteration of gradient descent. A larger learning rate can speed up convergence but may lead to overshooting the minimum. A smaller learning rate can lead to more stable convergence but may take longer to reach the minimum.\n",
    "\n",
    "5. **Types of Gradient Descent:**\n",
    "   - **Batch Gradient Descent:** It computes the gradient using the entire training dataset in each iteration. It can be slow for large datasets but usually provides more stable convergence.\n",
    "   - **Stochastic Gradient Descent (SGD):** It computes the gradient using a single randomly chosen data point in each iteration. This can be faster but may have more erratic convergence.\n",
    "   - **Mini-Batch Gradient Descent:** It falls between batch and stochastic gradient descent, where the gradient is computed using a small random subset (mini-batch) of the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1c231-a101-42f9-996d-cf4d09592899",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe01d42-5d87-4a61-a5f9-03f95f75479d",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that allows you to model the relationship between a dependent variable and multiple independent variables. While simple linear regression deals with a single independent variable, multiple linear regression can handle two or more independent variables. The key differences between the two models are as follows:\n",
    "\n",
    "**1. Number of Independent Variables:**\n",
    "\n",
    "- **Simple Linear Regression:** In simple linear regression, there is only one independent variable (X) and one dependent variable (Y). The relationship between Y and X is modeled as a straight line (linear relationship) represented by the equation Y = β₀ + β₁X + ε, where β₀ and β₁ are the model parameters (intercept and slope), and ε represents the error term.\n",
    "\n",
    "- **Multiple Linear Regression:** In multiple linear regression, there are two or more independent variables (X₁, X₂, ..., Xₙ) and one dependent variable (Y). The relationship between Y and the multiple independent variables is modeled as a linear equation represented by the equation Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ + ε. Here, β₀, β₁, β₂, ..., βₙ are the model parameters, each associated with a specific independent variable, and ε represents the error term.\n",
    "\n",
    "**2. Equation Complexity:**\n",
    "\n",
    "- **Simple Linear Regression:** The equation for simple linear regression is relatively straightforward, with only two parameters to estimate (intercept and slope).\n",
    "\n",
    "- **Multiple Linear Regression:** The equation for multiple linear regression becomes more complex as you introduce additional independent variables. You have to estimate a separate coefficient (β) for each independent variable.\n",
    "\n",
    "**3. Interpretation:**\n",
    "\n",
    "- **Simple Linear Regression:** The interpretation of the slope (β₁) in simple linear regression is relatively straightforward. It represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X), holding all other variables constant.\n",
    "\n",
    "- **Multiple Linear Regression:** In multiple linear regression, the interpretation of each coefficient (β) is more nuanced. Each β represents the change in Y for a one-unit change in the corresponding independent variable (X₁, X₂, etc.), while holding all other independent variables constant. This allows you to analyze the impact of each independent variable on the dependent variable while controlling for the effects of the other variables.\n",
    "\n",
    "**4. Complexity of Modeling Real-World Relationships:**\n",
    "\n",
    "- **Simple Linear Regression:** Simple linear regression is suitable when you want to model relationships between two variables, such as predicting a student's test score based on the number of hours they studied.\n",
    "\n",
    "- **Multiple Linear Regression:** Multiple linear regression is essential when you want to consider the influence of multiple factors simultaneously. For example, you might want to predict a person's income based on their education level, years of experience, and geographic location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ba796-7a92-42d4-bd56-277023a10b30",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0211ffe-de47-458d-89d9-eb100096bb32",
   "metadata": {},
   "source": [
    "Multicollinearity is a common issue that can arise in multiple linear regression when two or more independent variables in a regression model are highly correlated with each other. In other words, multicollinearity occurs when there is a strong linear relationship between two or more independent variables. This can lead to several problems in the regression analysis, including unstable coefficient estimates and difficulties in interpreting the individual contributions of the correlated variables. Here's a more detailed explanation of multicollinearity, how to detect it, and how to address it:\n",
    "\n",
    "**Causes of Multicollinearity:**\n",
    "1. **Data Collection:** Multicollinearity can arise from the way data is collected. For example, if you are predicting a person's weight, and you include both height in inches and height in centimeters as independent variables, these two variables will be highly correlated because they represent the same underlying concept.\n",
    "\n",
    "2. **Inherent Relationships:** In some cases, independent variables may naturally be related to each other. For example, in economics, variables like income and education level are often correlated because higher education tends to lead to higher income.\n",
    "\n",
    "**Effects of Multicollinearity:**\n",
    "1. **Unstable Coefficient Estimates:** Multicollinearity can result in unstable and imprecise coefficient estimates. Small changes in the data can lead to significant changes in the estimated coefficients.\n",
    "\n",
    "2. **Reduced Interpretability:** It becomes challenging to interpret the individual contributions of correlated variables to the dependent variable. This can make it difficult to identify which variables are truly important in the model.\n",
    "\n",
    "**Detecting Multicollinearity:**\n",
    "There are several methods to detect multicollinearity in a multiple linear regression model:\n",
    "\n",
    "1. **Correlation Matrix:** Calculate the correlation coefficients between all pairs of independent variables. If you find a high correlation (typically above 0.7 or 0.8) between two or more variables, it's a sign of potential multicollinearity.\n",
    "\n",
    "2. **Variance Inflation Factor (VIF):** VIF measures how much the variance of the estimated regression coefficients is increased due to multicollinearity. A VIF greater than 1 indicates some level of multicollinearity, with higher values indicating stronger multicollinearity.\n",
    "\n",
    "**Addressing Multicollinearity:**\n",
    "Once multicollinearity is detected, you can take several steps to address it:\n",
    "\n",
    "1. **Remove Redundant Variables:** If you have two or more variables that are highly correlated, consider removing one of them from the model. This reduces the multicollinearity but comes at the cost of losing information.\n",
    "\n",
    "2. **Combine Variables:** In some cases, you can create composite variables that capture the essence of the correlated variables. For example, if you have height in inches and height in centimeters, you can use just one of these variables.\n",
    "\n",
    "3. **Regularization:** Techniques like Ridge Regression or Lasso Regression can help mitigate multicollinearity by adding a penalty term to the regression model that discourages the coefficients from becoming too large.\n",
    "\n",
    "4. **Collect More Data:** Sometimes, increasing the size of the dataset can help reduce multicollinearity by providing more information to distinguish between correlated variables.\n",
    "\n",
    "5. **Principal Component Analysis (PCA):** PCA is a dimensionality reduction technique that can transform correlated variables into a set of linearly uncorrelated variables, known as principal components. You can use these components in your regression analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3dfc2-9849-40fd-a025-8dc652a51809",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f1112-b7e8-48d7-b179-70de6f2a703a",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis used when the relationship between the independent variable(s) and the dependent variable is not linear but instead follows a polynomial pattern. In contrast to linear regression, which models a linear relationship between variables, polynomial regression allows for more flexible modeling of non-linear relationships by fitting a polynomial equation to the data. Here's a description of polynomial regression and how it differs from linear regression:\n",
    "\n",
    "**Polynomial Regression:**\n",
    "\n",
    "In polynomial regression, the relationship between the dependent variable (Y) and one or more independent variables (X) is represented by a polynomial equation of a specified degree (e.g., quadratic, cubic, etc.). The general form of a polynomial regression equation with one independent variable is:\n",
    "\n",
    "Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βₖXᵏ + ε\n",
    "\n",
    "- Y is the dependent variable.\n",
    "- X is the independent variable.\n",
    "- β₀, β₁, β₂, ..., βₖ are the coefficients of the polynomial terms, which are estimated from the data.\n",
    "- ε represents the error term, which accounts for the unexplained variation in the dependent variable.\n",
    "\n",
    "**Differences from Linear Regression:**\n",
    "\n",
    "1. **Equation Form:**\n",
    "   - **Linear Regression:** Linear regression models the relationship between the dependent variable and the independent variable(s) using a linear equation of the form Y = β₀ + β₁X + ε. It assumes a straight-line relationship between variables.\n",
    "   - **Polynomial Regression:** Polynomial regression uses a polynomial equation with higher-order terms (e.g., X², X³, etc.), allowing it to capture non-linear patterns in the data.\n",
    "\n",
    "2. **Flexibility:**\n",
    "   - **Linear Regression:** Linear regression is appropriate for modeling linear relationships and is relatively inflexible when it comes to capturing non-linear patterns in the data.\n",
    "   - **Polynomial Regression:** Polynomial regression is more flexible and can model a wider range of relationships, including curves and bends in the data.\n",
    "\n",
    "3. **Model Complexity:**\n",
    "   - **Linear Regression:** Linear regression is simpler and has fewer model parameters (coefficients) to estimate. It assumes a constant rate of change.\n",
    "   - **Polynomial Regression:** Polynomial regression can be more complex, especially as the degree of the polynomial (k) increases. Higher degrees can result in overfitting if not carefully controlled.\n",
    "\n",
    "4. **Use Cases:**\n",
    "   - **Linear Regression:** Linear regression is suitable when you have reason to believe that the relationship between variables is linear, such as predicting house prices based on square footage.\n",
    "   - **Polynomial Regression:** Polynomial regression is used when there is evidence or intuition suggesting that the relationship is non-linear, such as modeling the growth of a plant over time, where the growth rate might initially be slow and then accelerate.\n",
    "\n",
    "5. **Interpretability:**\n",
    "   - **Linear Regression:** Coefficient interpretation in linear regression is straightforward. β₁ represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "   - **Polynomial Regression:** Interpretation of coefficients in polynomial regression becomes more complex as the degree of the polynomial increases, making it less intuitive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5178f1e-fc8d-4779-9941-232e4759fb07",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b94d01f-b256-4f8e-8453-3b7162a76892",
   "metadata": {},
   "source": [
    "Polynomial regression offers some advantages and disadvantages compared to linear regression, and the choice between the two depends on the nature of the data and the underlying relationships you want to model. Here are the advantages and disadvantages of polynomial regression, along with situations where it is preferable:\n",
    "\n",
    "**Advantages of Polynomial Regression:**\n",
    "\n",
    "1. **Flexibility:** Polynomial regression is highly flexible and can capture a wide range of non-linear relationships. It allows you to model curves, bends, and complex patterns in the data.\n",
    "\n",
    "2. **Improved Fit:** When the relationship between the independent and dependent variables is non-linear, polynomial regression often provides a better fit to the data compared to linear regression, which assumes linearity.\n",
    "\n",
    "3. **Better Predictive Power:** In cases where the underlying relationship is non-linear, using polynomial regression can result in improved predictive accuracy, especially when there are curvilinear trends in the data.\n",
    "\n",
    "**Disadvantages of Polynomial Regression:**\n",
    "\n",
    "1. **Overfitting:** Polynomial regression can be prone to overfitting, especially when using high-degree polynomials. Overfit models are excessively complex and may not generalize well to new, unseen data.\n",
    "\n",
    "2. **Interpretability:** As the degree of the polynomial increases, interpreting the coefficients becomes more challenging and less intuitive, making it harder to draw meaningful conclusions about the relationships between variables.\n",
    "\n",
    "3. **Computational Complexity:** Higher-degree polynomial regression models require more computational resources and may be slower to train than linear regression models. This can be a concern when dealing with large datasets.\n",
    "\n",
    "**When to Prefer Polynomial Regression:**\n",
    "\n",
    "You should consider using polynomial regression when:\n",
    "\n",
    "1. **Non-Linear Relationships:** You have reason to believe that the relationship between the independent and dependent variables is non-linear, and a straight line does not adequately represent the data.\n",
    "\n",
    "2. **Complex Patterns:** The data exhibits complex patterns, such as curves or bends, and you want to capture these patterns in your model.\n",
    "\n",
    "3. **Improved Predictive Accuracy:** You aim to achieve better predictive accuracy by accommodating non-linearities in the data.\n",
    "\n",
    "4. **Careful Regularization:** You can use techniques like Ridge Regression or Lasso Regression to mitigate overfitting when working with high-degree polynomials.\n",
    "\n",
    "5. **Interpretability is Not a Priority:** If your primary goal is prediction rather than interpretability, and you have a well-validated model, polynomial regression can be a useful tool.\n",
    "\n",
    "6. **Adequate Data:** Ensure that you have sufficient data to support the complexity of the polynomial model you intend to use. Overfitting is a risk when dealing with small datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f992208-5dbd-4e64-b174-cc61378ae11d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
